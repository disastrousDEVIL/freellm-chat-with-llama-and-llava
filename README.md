Sure! Here's **everything bundled into one complete `README.md`** in a single markdown block â€” ready to copy and paste into your GitHub root:

````markdown
# ðŸ§  FreeLLM Chat: LLaMA 3 + LLaVA Vision

A fully **local**, free-to-use chatbot that supports both **text and image-based queries**. Built with Flask, React, and powered by **LLaMA 3** and **LLaVA** via [Ollama](https://ollama.com/).

> ðŸ†“ No OpenAI API keys. No internet needed after setup. Just pure LLM magic on your machine.

---

## âš¡ Features

- ðŸ’¬ **Text chat** using LLaMA 3 (latest)
- ðŸ–¼ï¸ **Image + prompt chat** using LLaVA (vision model)
- ðŸ§  **Smart model switch**:  
  â†’ If user sends an image â†’ uses **LLaVA**  
  â†’ If only text â†’ uses **LLaMA 3**
- ðŸ” Maintains chat history during session
- ðŸŒ Frontend in **React**
- ðŸ–¥ï¸ Backend in **Flask**
- ðŸ“¦ Powered locally using **Ollama**

---

## ðŸ“¦ Tech Stack

- **Frontend**: React + Axios
- **Backend**: Python + Flask
- **LLM Runtime**: Ollama (LLaMA 3 & LLaVA)
- **Vision Support**: LLaVA (image understanding)

---

## ðŸš€ Getting Started

### âœ… Prerequisites

- Python 3.9+
- Node.js and npm
- Ollama (download from [ollama.com](https://ollama.com/))

---

### ðŸ”ƒ Pull Models with Ollama

```bash
ollama pull llama3
ollama pull llava
````

---

### ðŸ“ Clone the Repo

```bash
git clone https://github.com/disastrousDEVIL/freellm-chat-with-llama-and-llava.git
cd freellm-chat-with-llama-and-llava
```

---

### ðŸ§  Start the Backend (Flask)

```bash
cd llama-chat-backend
python -m venv venv
venv\Scripts\activate     # On Windows
# OR
source venv/bin/activate  # On macOS/Linux

pip install -r requirements.txt
python app.py
```

Runs on: [http://localhost:5000](http://localhost:5000)

---

### ðŸ’» Start the Frontend (React)

```bash
cd ../llama-chat-frontend
npm install
npm start
```

Runs on: [http://localhost:3000](http://localhost:3000)

---

## ðŸ¤– Model Selection Logic

| User Input    | Model Used              |
| ------------- | ----------------------- |
| Text only     | LLaMA 3                 |
| Image + Text  | LLaVA                   |
| â€œBestâ€ Option | Auto-detects best model |

---

## ðŸ—‚ï¸ Folder Structure

```
freellm-chat-with-llama-and-llava/
â”œâ”€â”€ llama-chat-backend/        # Flask API
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ llama-chat-frontend/       # React UI
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                  # â† You're here
â””â”€â”€ LICENSE
```

---

## ðŸ“ Notes

* âš ï¸ If you see `llama-chat-frontend` being treated as a submodule, make sure to delete its `.git` folder and re-add it as a normal directory.
* ðŸ§  Chat history is retained in-session but resets on page reload.
* ðŸ”§ Customize prompts or add role/system messages from the backend for smarter behavior.

---

## ðŸ” License

Licensed under the [MIT License](LICENSE).

> Youâ€™re free to use, modify, and distribute â€” just include the original license & credits.

---

## ðŸ™Œ Credits

* [Meta AI](https://ai.meta.com/llama/) for LLaMA 3
* [LLaVA Vision Model](https://llava-vl.github.io/)
* [Ollama](https://ollama.com) for local model serving

---

### ðŸŒŸ Star the repo if this helped you go serverless with LLMs â€” no API bills ever again!

```

Let me know if you'd like badges, demo screenshots, or a video preview section added next!
```
