# ğŸ–¼ï¸ Frontend â€“ FreeLLM Chat

This is the **frontend** for [FreeLLM Chat](https://github.com/disastrousDEVIL/freellm-chat-with-llama-and-llava), a 100% free, local chatbot powered by **LLaMA** and **LLaVA** using **Ollama** and **Flask**.  
No API keys. No cloud. Just chat.

---

## ğŸš€ Features

- ğŸ”¥ Clean and minimal chat UI
- ğŸ§  Works with both LLaMA (text) and LLaVA (vision) models
- ğŸ“¸ Image upload support (auto-routes to vision model)
- ğŸ§¾ Chat history maintained during session
- âš›ï¸ Built with React + Create React App

---

## ğŸ“¦ Tech Stack

- React (with Create React App)
- HTML/CSS
- Axios (for API calls)
- Runs on `localhost:3000`

---

## ğŸ› ï¸ Getting Started

Make sure the **backend (Flask + Ollama)** is running at `http://localhost:5000`.

### 1. Install dependencies
```bash
npm install
```

### 2. Start development server
```bash
npm start
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

---

## ğŸ§ª Available Scripts

These come from Create React App:

- `npm start` â€“ Start local server
- `npm test` â€“ Run tests
- `npm run build` â€“ Production build
- `npm run eject` â€“ Eject from CRA (not recommended)

---

## ğŸ“ Folder Structure (example)

```
llama-chat-frontend/
â”‚
â”œâ”€â”€ public/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ App.js
â”‚   â”œâ”€â”€ index.js
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ README.md  â† You are here
```

---

## ğŸ“ Notes

- The frontend assumes the backend is running on port `5000`.
- No authentication or database is required.
- Keep session short â€“ chat history is in-memory only.

---

Made with ğŸ’» for local LLM lovers.
